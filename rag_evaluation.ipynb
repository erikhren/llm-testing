{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALLS\n",
    "# llama-index-readers-file pymupdf\n",
    "# pip install llama-index-llms-azure-openai\n",
    "# ipykernel\n",
    "# pip install llama-index-llms-ollama\n",
    "\n",
    "# NOT YET\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "\n",
    "# https://docs.llamaindex.ai/en/stable/examples/low_level/oss_ingestion_retrieval/\n",
    "\n",
    "########\n",
    "\n",
    "# ! pip install langchain langchain-chroma \"unstructured[all-docs]\" pydantic lxml langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from typing import Any\n",
    "from backend.util.eval_util import ensure_model_available\n",
    "from pydantic import BaseModel\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull models\n",
    "llm_model = \"llama3.1:8b-chat\"\n",
    "ensure_model_available(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"mxbai-embed-large\"\n",
    "ensure_model_available(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05944c56",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/workspaces/llm-testing/backend/rag_data/llama2.pdf\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=PATH + \"LLaMA2.pdf\",\n",
    "    # Unstructured first finds embedded image blocks\n",
    "    extract_images_in_pdf=False,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de652659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "# TableChunk if Table > max chars set above\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a14a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(len(table_elements))\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606c7a8",
   "metadata": {},
   "source": [
    "## Multi-vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOllama(model=llm_model)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5556faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to text\n",
    "texts = [i.text for i in text_elements if i.text != \"\"]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45894aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f94fb",
   "metadata": {},
   "source": [
    "## Add to vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48241b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"summaries\", embedding_function=OllamaEmbeddings(model=embedding_model)\n",
    "    )\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()  # <- Can we extend this to images\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9a1a5",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "model = ChatOllama(model=\"llama2:13b-chat\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What is the number of training tokens for LLaMA2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fc36d",
   "metadata": {},
   "source": [
    "## Rag Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate questions and expected answers\n",
    "questions_and_answers = [\n",
    "    {\"question\": \"What is Llama 2?\", \"expected_answer\": \"Llama 2 is a collection of pretrained and fine-tuned large language models developed by Meta AI, ranging from 7B to 70B parameters.\"},\n",
    "    {\"question\": \"What datasets were used to train Llama 2?\", \"expected_answer\": \"Publicly available online data excluding Meta's own products or services.\"},\n",
    "    {\"question\": \"What size models are included in the Llama 2 release?\", \"expected_answer\": \"7B, 13B, and 70B parameter models.\"},\n",
    "    {\"question\": \"What is RLHF and how was it used in Llama 2-Chat?\", \"expected_answer\": \"Reinforcement Learning with Human Feedback was used after supervised fine-tuning to better align responses with human preferences.\"},\n",
    "    {\"question\": \"How does Llama 2 improve over Llama 1?\", \"expected_answer\": \"Better data cleaning, 40% more tokens, doubled context length, and Grouped-Query Attention.\"},\n",
    "    {\"question\": \"What special technique was introduced to maintain dialogue consistency over multiple turns?\", \"expected_answer\": \"Ghost Attention (GAtt).\"},\n",
    "    {\"question\": \"What steps were taken to ensure the safety of Llama 2?\", \"expected_answer\": \"Safety tuning, red teaming, safety-specific annotations, and safety reward models.\"},\n",
    "    {\"question\": \"What optimizer and learning schedule were used during pretraining?\", \"expected_answer\": \"AdamW optimizer with cosine learning rate schedule.\"},\n",
    "    {\"question\": \"What benchmarks does Llama 2 outperform its predecessors on?\", \"expected_answer\": \"MMLU, BBH, AGI Eval, and other academic benchmarks.\"},\n",
    "    {\"question\": \"What was the carbon footprint of training Llama 2 models?\", \"expected_answer\": \"539 tCO2eq emissions, 100% offset by Meta.\"},\n",
    "    {\"question\": \"Why is open-sourcing Llama 2 significant?\", \"expected_answer\": \"It promotes responsible, transparent AI development.\"},\n",
    "    {\"question\": \"What type of tokenization was used for Llama 2?\", \"expected_answer\": \"Byte-Pair Encoding (BPE) with 32k tokens.\"},\n",
    "    {\"question\": \"What are the two main objectives Llama 2-Chat was optimized for during RLHF?\", \"expected_answer\": \"Helpfulness and Safety.\"},\n",
    "    {\"question\": \"What is the Responsible Use Guide for Llama 2?\", \"expected_answer\": \"Guidelines provided to safely deploy Llama 2 and Llama 2-Chat.\"},\n",
    "    {\"question\": \"What is rejection sampling in Llama 2 fine-tuning?\", \"expected_answer\": \"Sampling multiple outputs and choosing the best using a reward model.\"},\n",
    "    {\"question\": \"What major safety evaluation metric was used for Llama 2-Chat?\", \"expected_answer\": \"Evaluation on ~2,000 adversarial prompts for safety violations.\"},\n",
    "    {\"question\": \"How does Llama 2-Chat compare to GPT-3.5 and GPT-4 on benchmarks?\", \"expected_answer\": \"Competitive with GPT-3.5 but still behind GPT-4.\"},\n",
    "    {\"question\": \"What is the purpose of training separate helpfulness and safety reward models?\", \"expected_answer\": \"To better specialize alignment without trade-offs between objectives.\"},\n",
    "    {\"question\": \"How did the authors address dataset contamination?\", \"expected_answer\": \"Through analyses to detect and limit overlap between training data and benchmarks.\"},\n",
    "    {\"question\": \"What is GQA and why is it important for Llama 2?\", \"expected_answer\": \"Grouped-Query Attention improves inference scalability in large models.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076996ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "csv_filename = \"rag_evaluation_results.csv\"\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Question\", \"Expected Answer\", \"Generated Answer\", \"Correctness\", \"Relevance\", \"Groundedness\", \"Retrieval relevance\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate through questions\n",
    "    for qa in questions_and_answers:\n",
    "        question = qa[\"question\"]\n",
    "        expected_answer = qa[\"expected_answer\"]\n",
    "\n",
    "        try:\n",
    "            generated_answer = chain.invoke(question)\n",
    "        except Exception as e:\n",
    "            generated_answer = f\"Error: {str(e)}\"\n",
    "\n",
    "        writer.writerow({\n",
    "            \"Question\": question,\n",
    "            \"Expected Answer\": expected_answer,\n",
    "            \"Generated Answer\": generated_answer,\n",
    "            \"Correctness\": \"\",\n",
    "            \"Relevance\": \"\",\n",
    "            \"Groundedness\": \"\",\n",
    "            \"Retrieval relevance\": \"\"\n",
    "        })\n",
    "\n",
    "print(f\"CSV file '{csv_filename}' has been created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02678954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.schema import TextNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f804d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = \"<api-key>\"\n",
    "# azure_endpoint = \"https://<your-resource-name>.openai.azure.com/\"\n",
    "# api_version = \"2023-07-01-preview\"\n",
    "\n",
    "# llm = AzureOpenAI(\n",
    "#     model=\"gpt-35-turbo-16k\",\n",
    "#     deployment_name=\"my-custom-llm\",\n",
    "#     api_key=api_key,\n",
    "#     azure_endpoint=azure_endpoint,\n",
    "#     api_version=api_version,\n",
    "# )\n",
    "llm = Ollama(model=\"llama2\", request_timeout=60.0)\n",
    "embed_model = Ollama(model=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26ba9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"backend/rag_data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=1024)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "index = VectorStoreIndex(nodes)\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cb397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "query_str = (\n",
    "    \"What is the specific name given to the fine-tuned LLMs optimized for\"\n",
    "    \" dialogue use cases?\"\n",
    ")\n",
    "\n",
    "generated_answer = str(query_engine.query(query_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54baac30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9145d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d449f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8b6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ceae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04096e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
